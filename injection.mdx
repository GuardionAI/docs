---
title: "Prompt Defense"
description: "Prompt Injection and Jailbreak detection powered by ModernGuard models"
---

The Injection detector protects your agents and applications from prompt injections, jailbreaks, context hijacking, and data exfiltration attempts. It is powered by the ModernGuard model family and is designed for low-latency, multilingual runtime use.

## What it detects

- Prompt injections and jailbreaks (e.g., DAN, Goodside)
- Context hijacking and instruction overrides
- Evasion and obfuscation (e.g., Unicode/ANSI/ASCII tricks)
- Data exfiltration and leakage attempts
- Code/command injection patterns (shell, SQL, tool abuse)

## Available models (versions)

- [ModernGuard v1.5](/modern-guard) — latest, recommended for production
- [ModernGuard v1](/modern-guard) — stable, production-proven
- [ModernGuard v0](/modern-guard) — initial release

See the detailed model card in [ModernGuard](/modern-guard) for architecture and benchmarks.

## Categories

The detector classifies threats across multiple categories you can target in policies:

- DIRECT_OVERRIDE
- OBFUSCATION
- CONTEXT_HIJACK
- DATA_EXFILTRATION
- ROLE_IMPERSONATION
- MULTISTEP_INSTRUCTION_HIDING
- TASK_MISUSE

## Using the Injection detector

You configure the detector via guard policies. First create a policy, then evaluate with that policy by overriding enabled policies.

```js
// 1) Create or update a Prompt Defense policy
await fetch("https://api.guardion.ai/v1/policies", {
  method: "POST",
  headers: {
    "Content-Type": "application/json",
    "Authorization": "Bearer YOUR_API_KEY"
  },
  body: JSON.stringify({
    id: "prompt-defense",
    definition: "Prevent prompt injections and jailbreaks",
    threshold: 0.9
    detector: {
      model: "modern-guard",
      target: "user",
    }
  })
});

// 2) Evaluate using that policy
const response = await fetch("https://api.guardion.ai/v1/guard", {
  method: "POST",
  headers: {
    "Content-Type": "application/json",
    "Authorization": "Bearer YOUR_API_KEY"
  },
  body: JSON.stringify({
    messages: [{ role: "user", content: "..." }],
    override_enabled_policies: ["prompt-defense"]
  })
});
```

### Threshold levels

- L1 (0.9): Confident
- L2 (0.8): Very Likely
- L3 (0.7): Likely
- L4 (0.6): Less Likely

Adjust thresholds per use case to balance false positives and coverage.

## Related

- ModernGuard — model card, benchmarks, and performance details
- Policies — how to compose and deploy guardrails

